{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deepfake Detector — Trimmed Demo Notebook\n",
    "# Paste cells into Colab or run this file as a notebook. This demo *loads a saved model* and\n",
    "# provides a Gradio UI for uploading a short video and receiving a video-level prediction.\n",
    "# It also includes a small evaluation cell to run over videos already in Drive.\n",
    "\n",
    "# 0) Install (run once in Colab)\n",
    "!pip install -q facenet-pytorch gradio torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Mount Google Drive and set paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "BASE = '/content/drive/MyDrive/deepfake_project'\n",
    "MODEL_PATH = os.path.join(BASE, 'models', 'best_resnet18_aug_balanced.pth')\n",
    "print('BASE:', BASE)\n",
    "print('MODEL_PATH:', MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Imports, device, transforms, MTCNN\n",
    "import torch, torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "mtcnn = MTCNN(image_size=IMG_SIZE, margin=30, keep_all=False, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d23a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load model (ResNet18 architecture matching training)\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print('Model loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Utility: predict per-face probabilities and aggregate to video-level\n",
    "import cv2\n",
    "\n",
    "def predict_frame_prob_facecrop(img_pil):\n",
    "    # img_pil = PIL Image\n",
    "    face_tensor = mtcnn(img_pil)\n",
    "    if face_tensor is None:\n",
    "        return None\n",
    "    t = val_transforms(Image.fromarray((face_tensor.permute(1,2,0).mul(255).byte().numpy()).astype(np.uint8)))\n",
    "    t = t.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(t)\n",
    "        prob = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "    return prob\n",
    "\n",
    "def video_to_prediction_facecrop(video_path, every_n=15, max_frames=120):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    probs = []\n",
    "    read = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % every_n == 0:\n",
    "            pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            face_tensor = mtcnn(pil)\n",
    "            if face_tensor is not None:\n",
    "                # predict using face crop\n",
    "                t = val_transforms(Image.fromarray((face_tensor.permute(1,2,0).mul(255).byte().numpy()).astype(np.uint8)))\n",
    "                t = t.unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    out = model(t)\n",
    "                    pr = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "                probs.append(pr)\n",
    "                read += 1\n",
    "                if max_frames and read >= max_frames:\n",
    "                    break\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    if not probs:\n",
    "        return None\n",
    "    avg = np.mean(probs, axis=0)\n",
    "    return avg  # returns [p_fake, p_real]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Gradio demo: upload a video -> returns video-level prediction and an example face crop\n",
    "import gradio as gr\n",
    "\n",
    "def video_predict_and_example(file, every_n=15, max_frames=120, thresh=0.7):\n",
    "    # file is a tempfile object or path\n",
    "    path = file if isinstance(file, str) else file.name\n",
    "    avg = video_to_prediction_facecrop(path, every_n=every_n, max_frames=max_frames)\n",
    "    if avg is None:\n",
    "        return (\"No faces detected\", None)\n",
    "    pf, pr = float(avg[0]), float(avg[1])\n",
    "    pred = 'fake' if pf >= thresh else 'real'\n",
    "    # also return one face crop as example (first detected)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frame_idx = 0\n",
    "    example_img = None\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        if frame_idx % every_n == 0:\n",
    "            pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            face_tensor = mtcnn(pil)\n",
    "            if face_tensor is not None:\n",
    "                arr = face_tensor.permute(1,2,0).mul(255).byte().numpy()\n",
    "                example_img = Image.fromarray(arr)\n",
    "                break\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    return (f\"pred: {pred}  prob_fake={pf:.3f} prob_real={pr:.3f}\", example_img)\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=video_predict_and_example,\n",
    "    inputs=[gr.Video(label='Upload MP4'), gr.Slider(5,30,value=15,label='every_n (frame stride)'), gr.Slider(10,300,value=120,label='max_frames'), gr.Slider(0.5,0.9,value=0.7,step=0.05,label='threshold for fake')],\n",
    "    outputs=[gr.Textbox(label='Result'), gr.Image(label='Example face crop')],\n",
    "    title='Deepfake Detector — Demo (face-crop aggregation)',\n",
    "    description='Uploads a short MP4, runs face detection + ResNet model on face crops, and returns a video-level prediction.'\n",
    ")\n",
    "\n",
    "# Run with share=True in Colab to get a shareable link\n",
    "iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de60440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Quick evaluation cell (run locally on Drive videos) — saves a CSV\n",
    "import glob, pandas as pd\n",
    "real_videos = sorted(glob.glob(os.path.join(BASE,'raw_videos','real','*.mp4')))\n",
    "fake_videos = sorted(glob.glob(os.path.join(BASE,'raw_videos','fake','*.mp4')))\n",
    "all_videos = [(p,'real') for p in real_videos] + [(p,'fake') for p in fake_videos]\n",
    "rows = []\n",
    "for path, true_label in all_videos:\n",
    "    avg = video_to_prediction_facecrop(path, every_n=15, max_frames=120)\n",
    "    if avg is None:\n",
    "        rows.append({'video_path':path, 'true_label':true_label, 'pred_class':None, 'prob_fake':None, 'prob_real':None})\n",
    "    else:\n",
    "        pf, pr = float(avg[0]), float(avg[1])\n",
    "        pred = 'fake' if pf >= 0.7 else 'real'\n",
    "        rows.append({'video_path':path, 'true_label':true_label, 'pred_class':pred, 'prob_fake':pf, 'prob_real':pr})\n",
    "df = pd.DataFrame(rows)\n",
    "out_csv = os.path.join(BASE, 'video_predictions_demo.csv')\n",
    "df.to_csv(out_csv, index=False)\n",
    "print('Saved:', out_csv)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
